<!-- auto-generated document -->
<html lang = "en">
	<head>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

		<!-- Bootstrap CSS -->
		<!-- <link href="https://bootswatch.com/4/solar/bootstrap.min.css" rel="stylesheet"> -->
		<link href="https://bootswatch.com/4/slate/bootstrap.min.css" rel="stylesheet">
		

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<!-- set picture -->
		<link rel="icon" href="https://vignette1.wikia.nocookie.net/harrypotter/images/2/23/Hallows.png/revision/latest?cb=20090309113642">

		<!-- set title on top of the tab -->
		<title>Q Learning: Starting From the Top</title>
	</head>
	<body>
		<center class="jumbotron">
			<h2> 
				Q Learning: Starting From the Top
			</h2>
		</center>
		<div class="container">
			<p>
				In this set of posts, I want to go over Q-learning (a form of reinforcement learning). To start, we could go in two directions. We could explore at the bottom and look at the math behind neural networks and Q-learning. Or we could start at the top and see the end result. We are going to go with the latter.
			</p>

			<br/>
			<center>
				<img src="images/mountain_car.jpg" width=400>
				<br>
				<p>
					<small>
						<br/>
						Figure 1: the mountain car environment.
					</small>
				</p>
			</center>

			<p>
				To do this we are going to need a few libraries and a testbed. To test, we are going to use 
				<a target="_blank" href="https://gym.openai.com/">OpenAI’s Gym</a> 
				and use 
				<a target="_blank" href="https://gym.openai.com/envs/MountainCar-v0/">MountainCar-V0.</a>
				In this environment, proposed by Andrew Moore in his Ph.D. thesis, the car must reach the flag seen in figure 1. The car, though, does not have enough acceleration to achieve this by just going forward. Instead, it must go back and forward, steadily gaining enough speed to reach the goal. This is a problem that can be solved simply with a rule-based agent, however, reinforcement approaches can struggle with this. You’ll soon see that the amount of episodes it takes for q-learning to solve this is more than expected. 
			</p>
			<p>
				We are going to need to install gym, 
				<a target="_blank" href="https://keras.io/">keras, </a>
				and 
				<a target="_blank" href="https://github.com/keras-rl/keras-rl">keras-rl. </a>
				All of these can be installed with <code>pip install X</code>. I would recommend using anaconda when possible. Keras is a library built to use TensorFlow, CNTK, or Theano. Each of these three are neural network libraries that we can use to define networks. Keras makes it easy to use any of these and provides a very intuitive way to define networks. Also, when installing Keras it may be configured to use a library for neural networks you do not want to use; if you encounter this issue, please visit this 
				<a target="_blank" href="https://keras.io/backend/">site.</a>
				Keras-RL is a reinforcement learning library built on top of Keras which allows us to run reinforcement algorithms on Keras networks for any gym environment. Meaning, the code is incredibly simple for this post. As a note, TensorFlow does provide Keras built in, but this will not work with Keras-RL. You need to use Keras for this stage.
			</p>
			<p>
				With that done, we can start building our example without having a clue about anything we’re doing. As a side note, I don’t know if this is necessarily a good thing because it allows people access to technology that can be used very unethically. Though in this case, we are using it to learn the basics and gather an intuition for Q-learning. The first thing we need to do is to create our learning environment:
			</p>

			<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f92672">import</span> <span style="color: #f8f8f2">gym</span>
<span style="color: #f8f8f2">env</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">gym</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">make(</span><span style="color: #e6db74">&#39;MountainCar-v0&#39;</span><span style="color: #f8f8f2">)</span>
</pre></div>
			
			<br/>
			<p>
				We can now define our network:
			</p>

			<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f92672">from</span> <span style="color: #f8f8f2">keras.models</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">Sequential</span>
<span style="color: #f92672">from</span> <span style="color: #f8f8f2">keras.layers</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">Dense,</span> <span style="color: #f8f8f2">Activation,</span> <span style="color: #f8f8f2">Flatten</span>
<span style="color: #f92672">from</span> <span style="color: #f8f8f2">keras.optimizers</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">Adam</span>

<span style="color: #f8f8f2">model</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">Sequential()</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Flatten(input_shape</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">(</span><span style="color: #ae81ff">1</span><span style="color: #f8f8f2">,)</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">env</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">observation_space</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">shape))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Dense(</span><span style="color: #ae81ff">128</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Activation(</span><span style="color: #e6db74">&#39;relu&#39;</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Dense(</span><span style="color: #ae81ff">64</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Activation(</span><span style="color: #e6db74">&#39;relu&#39;</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Dense(</span><span style="color: #ae81ff">32</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Activation(</span><span style="color: #e6db74">&#39;relu&#39;</span><span style="color: #f8f8f2">))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Dense(env</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">action_space</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">n))</span>
<span style="color: #f8f8f2">model</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">add(Activation(</span><span style="color: #e6db74">&#39;linear&#39;</span><span style="color: #f8f8f2">))</span>
</pre></div>
			
			<br/>
			<p>
				First, notice that the first layer of the model is based on the observation space of the environment. This is telling the neural network what kind of input it should be expecting. At the tail end, you have a a layer that has the size of the action space. This means that for every action possible, the network will have an output node. Each output node represents an action that can be taken and the node with the highest output value will be used as an action for the given step. The rest of the network has important details that we are going to ignore. We only want to cover the basics and we will come back as we leave the top view and approach the bottom.
			</p>

			<p>
				Now that we have a network and an environment, we need it to learn. Better put, we need to have our network play a bunch of games and update itself to play even better. To do this, as mentioned, we are going to use Q-learning that has been implemented in Keras-RL.

			</p>

			<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f92672">from</span> <span style="color: #f8f8f2">rl.memory</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">SequentialMemory</span>
<span style="color: #f92672">from</span> <span style="color: #f8f8f2">rl.policy</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">BoltzmannQPolicy</span>
<span style="color: #f92672">from</span> <span style="color: #f8f8f2">rl.agents</span> <span style="color: #f92672">import</span> <span style="color: #f8f8f2">DQNAgent</span>

<span style="color: #f8f8f2">dqn</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">DQNAgent(</span>
    <span style="color: #f8f8f2">model</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">model,</span> 
    <span style="color: #f8f8f2">nb_actions</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">env</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">action_space</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">n,</span> 
    <span style="color: #f8f8f2">memory</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">SequentialMemory(limit</span><span style="color: #f92672">=</span><span style="color: #ae81ff">50000</span><span style="color: #f8f8f2">,</span> <span style="color: #f8f8f2">window_length</span><span style="color: #f92672">=</span><span style="color: #ae81ff">1</span><span style="color: #f8f8f2">),</span> 
    <span style="color: #f8f8f2">nb_steps_warmup</span><span style="color: #f92672">=</span><span style="color: #ae81ff">10</span><span style="color: #f8f8f2">,</span>
    <span style="color: #f8f8f2">target_model_update</span><span style="color: #f92672">=</span><span style="color: #ae81ff">1e-2</span><span style="color: #f8f8f2">,</span> 
    <span style="color: #f8f8f2">policy</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">BoltzmannQPolicy())</span>

<span style="color: #f8f8f2">dqn</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">compile(Adam(lr</span><span style="color: #f92672">=</span><span style="color: #ae81ff">1e-3</span><span style="color: #f8f8f2">),</span> <span style="color: #f8f8f2">metrics</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">[</span><span style="color: #e6db74">&#39;mae&#39;</span><span style="color: #f8f8f2">])</span>
<span style="color: #f8f8f2">dqn</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">fit(env,</span> <span style="color: #f8f8f2">nb_steps</span><span style="color: #f92672">=</span><span style="color: #ae81ff">150000</span><span style="color: #f8f8f2">,</span> <span style="color: #f8f8f2">visualize</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">False,</span> <span style="color: #f8f8f2">verbose</span><span style="color: #f92672">=</span><span style="color: #ae81ff">2</span><span style="color: #f8f8f2">)</span>
<span style="color: #f8f8f2">dqn</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">save_weights(</span><span style="color: #e6db74">&#39;model.mdl&#39;</span><span style="color: #f8f8f2">,</span> <span style="color: #f8f8f2">overwrite</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">True)</span>
<span style="color: #f8f8f2">dqn</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">test(env,</span> <span style="color: #f8f8f2">nb_episodes</span><span style="color: #f92672">=</span><span style="color: #ae81ff">5</span><span style="color: #f8f8f2">,</span> <span style="color: #f8f8f2">visualize</span><span style="color: #f92672">=</span><span style="color: #f8f8f2">True)</span>
</pre></div>
	
			<br/>
			<p>
				In this block of code, there is a lot of things that, likely, will not make sense. For example, what is the <code>BoltzmanQPolicy</code>? At the moment, it is isn’t important so treat it like a 
				<a target="_blank" href="https://en.wikipedia.org/wiki/Black_box">black box.</a>
				What is important in this block of code is that we can see the agent runs for 150,000 steps. Each game is composed of 200 steps which mean we give the agent 750 games to learn how to play the game. In my brief experiments with how many steps were necessary, this seemed to be the right amount. You’ll also notice that we have <code>visualize</code> while training set to false. This makes it so gym does not render every game which saves compute and will speed up our training time. After training, we save the model and run a test to see how it works. 
			</p>

			<center>
				<img src="images/training_car.gif" width=400 loop=true>
				<br>
				<p>
					<small>
						<br/>
						Figure 2: an example of the agent successfully reaching the target.
					</small>
				</p>
			</center>

			<p>
				If all went well, you’ll have a result similar to figure 2. The source code of this project can be found on 
				<a target="_blank" href="https://github.com/bi3mer/challenges/blob/master/Challenge040_KerasRLMountainCar/train.py">GitHub.</a>
				You’ll notice I added command line arguments and some functions to make the code cleaner and easier to use when experimenting. In the next post, we are going to peel back the first layer and look into q-learning. We will either implement it and then go over the theory or do the opposite. I’m not sure yet which is best yet, so I’m not committing to either. In the meantime, I’d recommend playing with the variables to gain an intuition for reinforcement learning. Try to get a feel for why the agents in bigger examples like 
				<a target="_blank" href="https://blog.openai.com/openai-five/">OpenAI’s Dota 2 bot</a>
				or 
				<a target="_blank" href="https://en.wikipedia.org/wiki/AlphaGo_Zero">DeepMind’s AlphaZero</a>
				required so many computers to learn how to play these more complicated games.
			</p>
		</div>

<div class="container">
  <h3 class="card mb-3">
    <div class="container" id="nextTime">
      Citation
    </div>
  </h3>
  <pre id="citation"> </pre>
  <br/>
  <br/>
</div>
<script>
(() => {
  const title = document.title;
  const journal = 'colan.biemer.us';
  const url = window.location.href;
  const year = url.split('=').pop();

  // remove special characters and replace spaces with underscores
  const tag = document.title.replace(/[^\w\s]/gi, '').split(' ').join('_');
  
  let citationText = `@article{biemer${year}${tag}\n`;
  citationText = `${citationText}  title="${title}",\n`;
  citationText = `${citationText}  author="F. Biemer, Colan",\n`;
  citationText = `${citationText}  journal="${journal}",\n`;
  citationText = `${citationText}  year="${year}",\n`;
  citationText = `${citationText}  url="${url}",\n}`;

  $('#citation').text(citationText);
})();
</script><!-- Disqus for comments -->
<div id="disqus_thread" class="container"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
	var d = document, s = d.createElement('script');
	s.src = 'https://bi3mer-github-io.disqus.com/embed.js';
	s.setAttribute('data-timestamp', +new Date());
	(d.head || d.body).appendChild(s);
})();
</script>
<noscript>
	Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<script id="dsq-count-scr" src="//bi3mer-github-io.disqus.com/count.js" async></script><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GA_TRACKING_ID"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VEPF12WK7P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VEPF12WK7P');
</script>	</body>
</html>
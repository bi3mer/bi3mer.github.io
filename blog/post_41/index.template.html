<html lang = "en">
	<head>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

		<!-- Bootstrap CSS -->
		<!-- <link href="https://bootswatch.com/4/solar/bootstrap.min.css" rel="stylesheet"> -->
		<link href="https://bootswatch.com/4/slate/bootstrap.min.css" rel="stylesheet">
		

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<!-- set picture -->
		 

		<!-- set title on top of the tab -->
		<title>Active RL: Q-Learning and Sarsa</title>
	</head>
	<body>
		<center class="jumbotron">
			<h2> 
				Active RL: Q-Learning and Sarsa
			</h2>
		</center>
		<div class="container">

		    <p>
		        This is part of a series of posts on reinforcement learning and Markov Decision Processes (MDP) (<a href="../post_36/index.html">Direct Utility Estimation</a>, <a href="../post_37/index.html">Updated MDP</a>, <a href="../post_38/index.html">Value Iteration</a>, <a href="../post_39/index.html">Policy Iteration</a>), and <a href="../post_39/index.html">Temporal Difference Learning</a>), and in this post I'm going to be covering two active reinforcement learning methods: q-learning and SARSA.
		   </p>

		   <p>
		   		Both methods do not depend on an MDP. Meaning, we are not guaranteed the presence of <code>P</code>. Instead, we learn which actions can be taken in a state during playthroughs. This is represented by a q-table, which is a table of states and actions that yield a q-value, which represents expected utility when taking an action in a given state. We can say that the expected utility of a state is the best action associated with that state. See the equation below.
		   </p>

		   $$
		   U(s) = \max_a Q(s,a)
		   $$

		   <p>
		   		In active reinforcement learning, we are updating the q-table after the agent makes a decision on the next move to make. To get the next move, you can use an epsilon greedy approach like in the code below or you can use the q-values in the q-table as weights for a probability-based selection. Note that a valid strategy is a greedy q-learning agent where only the <code>max_next</code> action selection strategy is used. This will method will converge quicker, but at the cost of likely finding a sub-optimal policy. 
		   </p>

		   <!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #66d9ef">def</span> <span style="color: #a6e22e">max_next</span><span style="color: #f8f8f2">(self,</span> <span style="color: #f8f8f2">s):</span>
    <span style="color: #f8f8f2">valid_choices</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">[new_s</span> <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">new_s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s]]</span>
    <span style="color: #f8f8f2">best_s</span> <span style="color: #f92672">=</span> <span style="color: #66d9ef">None</span>
    <span style="color: #f8f8f2">best_u</span> <span style="color: #f92672">=</span> <span style="color: #f92672">-</span><span style="color: #f8f8f2">inf</span>

    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">new_s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">valid_choices:</span>
        <span style="color: #f8f8f2">next_u</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][new_s]</span>
        <span style="color: #66d9ef">if</span> <span style="color: #f8f8f2">next_u</span> <span style="color: #f92672">&gt;</span> <span style="color: #f8f8f2">best_u:</span>
            <span style="color: #f8f8f2">best_u</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">next_u</span>
            <span style="color: #f8f8f2">best_s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">new_s</span>

    <span style="color: #66d9ef">return</span> <span style="color: #f8f8f2">best_s,</span> <span style="color: #f8f8f2">best_u</span>
    
<span style="color: #66d9ef">def</span> <span style="color: #a6e22e">get_next</span><span style="color: #f8f8f2">(self,</span> <span style="color: #f8f8f2">s,</span> <span style="color: #f8f8f2">eps</span><span style="color: #f92672">=</span><span style="color: #ae81ff">0.05</span><span style="color: #f8f8f2">):</span>
    <span style="color: #66d9ef">if</span> <span style="color: #f8f8f2">random()</span> <span style="color: #f92672">&lt;</span> <span style="color: #f8f8f2">eps:</span>
        <span style="color: #f8f8f2">best_s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">choice([new_s</span> <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">new_s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s]])</span>
        <span style="color: #f8f8f2">best_q</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][best_s]</span>
    <span style="color: #66d9ef">else</span><span style="color: #f8f8f2">:</span>
        <span style="color: #f8f8f2">best_s,</span> <span style="color: #f8f8f2">best_q</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">max_next(s)</span>

    <span style="color: #66d9ef">return</span> <span style="color: #f8f8f2">best_s,</span> <span style="color: #f8f8f2">best_q</span>
</pre></div>


		<br/>

		<p>
			With an action selected, we can update the q-table. We do this with the following equation:
		</p>


	   $$
	   Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma \max_a Q(s',a') - Q(s,a))
	   $$

	   <p>
	   	As you can see, there is a look ahead here. Given the current state and action to be taken, we say that its q-value is dependent on <code>s'</code> which is the result of taking action <code>a</code> in state <code>s</code>, where we find the optimal action to take in <code>s'</code> based on the q-table. This equation is almost exactly the same as what we saw in temporal difference learning.
	   </p>

	   <!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #66d9ef">def</span> <span style="color: #a6e22e">train</span><span style="color: #f8f8f2">(self,</span> <span style="color: #f8f8f2">max_iterations):</span>
    <span style="color: #f8f8f2">GAMMA</span> <span style="color: #f92672">=</span> <span style="color: #ae81ff">0.9</span>

    <span style="color: #f8f8f2">N</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">{}</span>
    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">S:</span>
        <span style="color: #f8f8f2">N[s]</span> <span style="color: #f92672">=</span> <span style="color: #ae81ff">1</span>

    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">trange(max_iterations):</span>
        <span style="color: #f8f8f2">s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">START</span>
        <span style="color: #66d9ef">while</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">not</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">E:</span>
            <span style="color: #f8f8f2">N[s]</span> <span style="color: #f92672">+=</span> <span style="color: #ae81ff">1</span>
            <span style="color: #f8f8f2">new_s,</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">get_next(s)</span>

            <span style="color: #f8f8f2">ALPHA</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">(</span><span style="color: #ae81ff">60.0</span><span style="color: #f92672">/</span><span style="color: #f8f8f2">(</span><span style="color: #ae81ff">59.0</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">N[s]))</span>
            <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][new_s]</span> <span style="color: #f92672">+=</span> <span style="color: #f8f8f2">ALPHA</span><span style="color: #f92672">*</span><span style="color: #f8f8f2">(self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">R[s]</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">GAMMA</span><span style="color: #f92672">*</span><span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">u(new_s)</span> <span style="color: #f92672">-</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][new_s])</span>
            <span style="color: #f8f8f2">s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">new_s</span>

        <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][</span><span style="color: #ae81ff">0</span><span style="color: #f8f8f2">]</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">R[s]</span>
</pre></div>
		<br/>

			<p>
				That is q-learning in a nutshell, and now I'm going to turn my attention to SARSA which received its name for <code>s,a,r,s',a</code>. In SARSA, we use a very similar seeming equation.
			</p>

			$$
			Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma Q(s',a') - Q(s,a))
			$$

			<p>
				The difference is that in q-learning we don't use the best action in the environment but we do use it in calculating the q-value. In SARSA, we don't use the max value but we do take whatever action is selected. This is why q-learning is considered an off-policy algorithm and SARSA is on-policy: the calculation of the q-values is based on the policy. In a sense, SARSA is learning what will actually happen whereas q-learning will eventually learn to behave well regardless of the policy, which is why SARSA is the better choice when we care about performance while the agent is learning. 
			</p>

			<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #66d9ef">def</span> <span style="color: #a6e22e">train</span><span style="color: #f8f8f2">(self,</span> <span style="color: #f8f8f2">max_iterations):</span>
    <span style="color: #f8f8f2">GAMMA</span> <span style="color: #f92672">=</span> <span style="color: #ae81ff">0.9</span>

    <span style="color: #f8f8f2">N</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">{}</span>
    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">S:</span>
        <span style="color: #f8f8f2">N[s]</span> <span style="color: #f92672">=</span> <span style="color: #ae81ff">1</span>

    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">trange(max_iterations):</span>
        <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">reset()</span>
        <span style="color: #f8f8f2">s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">START</span>
        <span style="color: #f8f8f2">s_1,</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">get_next(s)</span>
        <span style="color: #66d9ef">while</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">not</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">E:</span>
            <span style="color: #f8f8f2">N[s]</span> <span style="color: #f92672">+=</span> <span style="color: #ae81ff">1</span>
            <span style="color: #f8f8f2">s_2,</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">get_next(s_1)</span>

            <span style="color: #f8f8f2">ALPHA</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">(</span><span style="color: #ae81ff">60.0</span><span style="color: #f92672">/</span><span style="color: #f8f8f2">(</span><span style="color: #ae81ff">59.0</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">N[s]))</span>
            <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][s_1]</span> <span style="color: #f92672">+=</span> <span style="color: #f8f8f2">ALPHA</span><span style="color: #f92672">*</span><span style="color: #f8f8f2">(self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">R[s]</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">GAMMA</span><span style="color: #f92672">*</span><span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s_1][s_2]</span> <span style="color: #f92672">-</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][s_1])</span>

            <span style="color: #f8f8f2">s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">s_1</span>
            <span style="color: #f8f8f2">s_1</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">s_2</span>

        <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">Q[s][</span><span style="color: #ae81ff">0</span><span style="color: #f8f8f2">]</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">R[s]</span>
</pre></div>


			<br/>

			<p>
				Running both of these algorithms on GridWorld with a 20x20 grid, they were able to find a solution after about 600 full playthroughs the majority of the time. Q-learning was slower by about a tenth of a second on successful runs on these successful runs. This makes sense because once SARSA requires less computation since it doesn't compute <code>U(s,a)</code>. 
			</p>
			<p>
				I hope you enjoyed this post. You can find the code on <a href="https://github.com/bi3mer/ADP_Test/tree/978f51048ad055dabd7e37866159fb5d28b18452">GitHub.</a>
			</p>



		</div>
		<br/>
		{{../../comments.plugin.html}}
		{{../../google_analytics.plugin.html}}
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
	</body>
</html>
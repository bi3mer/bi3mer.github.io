<!-- auto-generated document -->
<html lang = "en">
	<head>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

		<!-- Bootstrap CSS -->
		<!-- <link href="https://bootswatch.com/4/solar/bootstrap.min.css" rel="stylesheet"> -->
		<link href="https://bootswatch.com/4/slate/bootstrap.min.css" rel="stylesheet">
		

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<!-- set picture -->
		<link rel="icon" href="https://vignette1.wikia.nocookie.net/harrypotter/images/2/23/Hallows.png/revision/latest?cb=20090309113642">

		<!-- set title on top of the tab -->
		<title>Policy Iteration</title>
	</head>
	<body>
		<center class="jumbotron">
			<h2> 
				Policy Iteration
			</h2>
		</center>
		<div class="container">

    <p>
        This is part of a series of posts on reinforcement learning and Markov Decision Processes (MDP) (<a href="../post_36/index.html">Direct Utility Estimation</a>, <a href="../post_37/index.html">Updated MDP</a>, and <a href="../post_38/index.html">Value Iteration</a>), and in this post I'll be covering policy iteration. As a brief reminder, here is the bellman equation that value iteration optimizes:
   </p>

   $$
   U(s) = R(s) + \gamma \max_{s \in S} \sum_{s'} P(s' | s, a)U(s')
   $$

   <p>
   		In policy iteration, the goal is not to find the perfectly optimal utility of states like value iteration. If one state is clearly better than another, then the precise difference isn't that important. After all, we care most about an agent making the correct decisions. This idea is how we come to policy iteration, which is broken into two steps: (1) <b>policy evaluation</b> and (2) <b>policy improvement</b>. Before going into each of these ideas, I want to give you the pseudo-code for the algorithm.
   </p>

<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f8f8f2">loop</span>
    <span style="color: #f8f8f2">policy_evaluation(policy,</span> <span style="color: #f8f8f2">U,</span> <span style="color: #f8f8f2">mdp)</span>
    <span style="color: #f8f8f2">unchanged</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">policy_improvement(policy,</span> <span style="color: #f8f8f2">U,</span> <span style="color: #f8f8f2">mdp)</span>
<span style="color: #66d9ef">while</span> <span style="color: #f8f8f2">unchanged</span> <span style="color: #f92672">is</span> <span style="color: #f8f8f2">false</span>
</pre></div>

	<br/>

	<p>
		A policy defines the best action for a given state. It can be deterministic (only one action is ever selected for a given state <code>s</code>) or stochastic (actions have probabilities of being selected). In grid world, the environment is deterministic so it is best to use a deterministic policy. For policy iteration, we initialize a policy with random actions for each state.
	</p>


	<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">{}</span> 
<span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">S:</span>
    <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi[s]</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">choice(list(self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">P[s]</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">keys()))</span>
</pre></div>

	<br/>


	<p>
		Policy evaluation is attempting to find the optimal policy for a problem and uses a modified Bellman equation that is run <code>k</code> times. 
	</p>

	$$
	U_i(s) = R(s) + \gamma \sum_{s'} P(s' | s, \pi(s))U_i(s')
	$$


	<p>
		The utility of a state is determined by the policy. Since our policy deterministic, the utility of a state is calculated with only one neighbor. If the policy was stochastic, we would multiply each action by the probability of it being selected by the policy.
	</p>

	<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #75715e"># policy evaluation</span>
<span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">_</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">range(K):</span>
    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">S:</span>
        <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">utility[s]</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">R[s]</span> <span style="color: #f92672">+</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">P[s][self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi[s]]</span> <span style="color: #f92672">*</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">utility[self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi[s]]</span>
</pre></div>

	<br/>

	<p>
		Next is policy improvement which is very simple. For every state <code>s</code>, we compare the utility of the action found by the policy to the best neighbor. If the utility of the policy is better than no changes are made to the policy. If, however, the policy does not match the utility table, we update the policy to select the better neighbor and we store that the algorithm has not yet converged. Note that policy improvement does not break out of the loop once a change to the policy has been found; all changes are run and the loop goes back up to policy improvement. 
	</p>

	<!-- HTML generated using hilite.me --><div style="background: #272822; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #f8f8f2">unchanged</span> <span style="color: #f92672">=</span> <span style="color: #66d9ef">True</span>
<span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">S:</span>
    <span style="color: #f8f8f2">old</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi[s]</span>

    <span style="color: #f8f8f2">best_s</span> <span style="color: #f92672">=</span> <span style="color: #66d9ef">None</span>
    <span style="color: #f8f8f2">best_u</span> <span style="color: #f92672">=</span> <span style="color: #f92672">-</span><span style="color: #f8f8f2">inf</span>
    <span style="color: #66d9ef">for</span> <span style="color: #f8f8f2">s_p</span> <span style="color: #f92672">in</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">P[s]:</span>
        <span style="color: #66d9ef">if</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">utility[s_p]</span> <span style="color: #f92672">&gt;</span> <span style="color: #f8f8f2">best_u:</span>
            <span style="color: #f8f8f2">best_s</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">s_p</span>
            <span style="color: #f8f8f2">best_u</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">utility[s_p]</span>

    <span style="color: #66d9ef">if</span> <span style="color: #f8f8f2">old</span> <span style="color: #f92672">!=</span> <span style="color: #f8f8f2">best_s:</span>
        <span style="color: #f8f8f2">self</span><span style="color: #f92672">.</span><span style="color: #f8f8f2">pi[s]</span> <span style="color: #f92672">=</span> <span style="color: #f8f8f2">best_s</span>
        <span style="color: #f8f8f2">unchanged</span> <span style="color: #f92672">=</span> <span style="color: #66d9ef">False</span>
</pre></div>

<br/>

	<p>
		That is policy iteration, the code can be found on <a href="https://github.com/bi3mer/ADP_Test/tree/d7c9a28c61ad8fbf7a0ec9295fed837d74a51c26">GitHub</a>, and now we can look at how the algorithm does. 
	</p>


	<center>
        $$t=100$$
        <table border="1">
            <tr>
                <th><small>70.95</small></th>
                <th><small>71.95</small></th>
                <th><small>72.95</small></th>
                <th><small>73.95</small></th>
            </tr>
            <tr>
                <th><small>69.95</small></th>
                <th style="background-color: black;"></th>
                <th><small>71.95</small></th>
                <th><small>72.00</small></th>
            </tr>
            <tr>
                <th><small>68.95</small></th>
                <th><small>69.95</small></th>
                <th><small>70.95</small></th>
                <th><small>71.00</small></th>
            </tr>
        </table>
    </center>
    <br/>

    <p>
    	Recall that value iteration converged after 13 iterations and this algorithm is taking 100, but that was due to the stopping criteria. In that example, I gave value iteration a theta of 0.03. If I increase the size of grid world to 10x10, then theta is large enough that value iteration fails to find a suitable solution. I have to give it <code>theta = 1e-10</code> for value iteration to work. and it takes 81 iterations. Policy iteration actually takes more iterations at 300, but it is faster in terms of time taken at 0.049 seconds compared to 0.056 seconds, which is in line with the common wisdom that policy iteration is quicker to converge. On the note of convergence, a problem I ran into with value iteration is that it eventually fails to find solutions given a large enough grid world. At a certain point, you can only make theta so small, whereas the policy approach had no such issue. Likely there are other convergence metrics you can use for value iteration, but I have not yet looked into them. 
    </p>

    <br/>
    <br/>
    <b>EDIT 2021/02/03:</b> <code>self.utility[s] = self.R[s] + self.P[s][self.pi[s]] * self.utility[self.pi[s]]</code> should be <code>self.utility[s] = self.R[s] + GAMMA*self.P[s][self.pi[s]] * self.utility[self.pi[s]]</code>, which has gamma. This mistake is why the table above has such large values. Policy iteration also converges after 80 iterations for <code>1e-13</code>.


		</div>
		<br/>
<!-- Disqus for comments -->
<div id="disqus_thread" class="container"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
	var d = document, s = d.createElement('script');
	s.src = 'https://bi3mer-github-io.disqus.com/embed.js';
	s.setAttribute('data-timestamp', +new Date());
	(d.head || d.body).appendChild(s);
})();
</script>
<noscript>
	Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<script id="dsq-count-scr" src="//bi3mer-github-io.disqus.com/count.js" async></script><!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GA_TRACKING_ID"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VEPF12WK7P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VEPF12WK7P');
</script>        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
	</body>
</html>
+++
date = '2025-03-19T13:02:08-04:00'
draft = true
title = 'Graph Simplfication for a Faster A*'
+++

[A*](https://en.wikipedia.org/wiki/A*_search_algorithm) is an algorithm that is so well covered that I won't be wasting my time to add to the noise in this blog post, but if you are unfamiliar with A*, I recommend reading an [introduction from Red Blob Games](https://www.redblobgames.com/pathfinding/a-star/introduction.html) and then coming back here.[^0]

Alright, assuming you are familiar with A*, the question you may be having is: "How can we make A* faster?" There are, in fact, a lot of ways. If you don't care about finding an optimal path, you can use an inadmissable heuristic.[^1] If you wish to reduce resources required, you can cache common path requests. If you have a problem where the path is constantly being recalculated, then you can migrate to [D*](https://en.wikipedia.org/wiki/D*). If you want every search possible to be less costly to compute while still being optimal, you have to do something diffeerent. Specifically, you need to reduce the search space.[^2]

Because this is a blog post, I am not going to cover all the related work. In fact, I made the mistake of not doing any searching beforehand when I came up with this method. From what I can tell now, after the fact, this method is not new or particularly innovative. It is, though, in my opinion, cool. The way it works is by taking a graph and simplifying it by turning it into a graph of graphs. Then, A* runs on the top level graph, before going into the subgraph. The result, as we'll see, is a faster A*. The cost, though, is that you have to run graph simplification offline. If you have a graph that is constantly changing, this is not the method for you.

The rest of this post is broken into sections. In the first, I show how mazes are generated. In the second, I describe the algorithm for simplifying grpahs. In the third, I show the results and discuss them.

## Generating Mazes

The code below is for generating mazes. You can see from the constants at the top that we are generating 10 mazes, with dimensions of 250x250. For the grpah,[NetworkX](https://networkx.org/) is used. A `grid_graph` is used to create the graph, and then the maze is generated by using a [minimum spanning tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree#:~:text=A%20minimum%20spanning%20tree%20). The default for this is to use [Kruskal's Algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm).[^3] Note, though, that beforehand, the weight of every edge in the graph is set randomly. This makes it so each maze generated is different.

```python
import networkx as nx
from random import random

def generate_maze(grid_size: int) -> nx.Graph:
    G = nx.grid_graph((grid_size, grid_size))

    for u,v in G.edges:
        G.edges[(u,v)]['weight'] = random()

    return nx.minimum_spanning_tree(G)
```

## Simplifying Graphs

The method I came up with—I am not saying that I am the first to have this idea—is that a graph can be simplified by looking for `critical nodes` and condensing the rest down to a subgraph. A critical node is any node that has a degree of greater than two. A nodes degree is determined by the number of incoming and outgoing edges.

The code (see below) starts by finding all such critical nodes, and intializing a `node_lookup` dictionary. The dictionary is not required, but it is useful. Take a simple example where we want to find a path fron node `A` to node `C`. In a regular graph, we could look up both by name in an 0(1) operation and everything is hunkydory. If, though, either `A` or `C` are in a sub-graph, we're in trouble. We'll have to look through every node until we find what we're looking for. The `node_lookup` dictionary address this by allowing us to again have an 0(1) operation.

Now, onto the actual algorithm. It starts by looping through every critical node and updating the lookup table. The next step is to loop through the edges that are connected to the critical node. For each of these is follows the edges until another critical node is found. Then, every node found is condensed into one sub-graph. The subgraph replaces the original set of uncritical nodes. Then an edge from the original node to the subraph is made, and an edge from the subraph to the newly found critical node is added if a critical node was found—if a critical node was not found, then no outgoing edge is added because the last node was a [leaf](https://proofwiki.org/wiki/Definition:Tree_(Graph_Theory)/Leaf_Node). Finally, the node lookup table is updated, and that is it. A pretty simple algorithm, at least I think.

```python
def build_hyper_graph(G: nx.Graph):
    critical_nodes = [n for n in G.nodes() if G.degree(n) > 2]
    node_lookup = {}

    for n in critical_nodes:
        node_lookup[n] = n
        next_nodes = list(G.edges(n))

        for _, a_2 in next_nodes:
            if a_2 in critical_nodes:
                continue

            if not G.has_edge(n, a_2):
                continue

            G.remove_edge(n, a_2)
            weight = 0
            nodes = [a_2]
            a_1 = n
            a_3 = None
            while True:
                if a_2 in critical_nodes:
                    nodes.pop()
                    break

                edges = G.edges(a_2)
                if len(edges) == 0:
                    G.remove_node(a_2)
                    a_1 = None
                    break

                for _, a_3 in G.edges(a_2):
                    if a_1 != a_3:
                        break

                weight += G.edges[(a_2, a_3)][W]
                nodes.append(a_3)
                G.remove_edge(a_2, a_3)
                G.remove_node(a_2)

                a_1 = a_2
                a_2 = a_3

            hyper_state_name = '||'.join(str(node_name) for node_name in nodes)
            for node_name in nodes:
                node_lookup[node_name] = hyper_state_name

            critical_nodes.append(hyper_state_name)
            G.add_edge(n, hyper_state_name, weight=weight, color='BLACK')

            if a_1 != None:
                G.add_edge(hyper_state_name, a_3, weight=0, color='BLACK')

    return node_lookup
```

## Results

|  |  |
|--|--|
|![alt text](/images/hypergraph/maze.png "Original maze") | ![alt text](/images/hypergraph/simple_maze.png "Simplified maze") |

In the first figure above, you can see a simple example of what happens when we take a big maze where the red line marks the solution, and we simplify it and run A* again. Just from a visual, it can be seen that the search space is reduced. But, by how much?

|  |  |
|--|--|
|![alt text](/images/hypergraph/nodes.png "Reduction for nodes.") | ![alt text](/images/hypergraph/edges.png "Reduction for edges.") |

As you can see, the larger the graph becomes, the more signifigant the reduction in nodes and edges.[^4] But, how much does this affect the overall speed of A*?

![alt text](/images/hypergraph/G_HG.png "A* solution time.")

We again find similar results where the larger the graph becomes, the more siginfigant the overall reduction in time spent running A*. Note, though, this final plot is a benchmark that is imprecise. I did not run on a server. I did not use a well-established performance clock. I did not do many of the thing sthat one should do if they are going to make claims regarding the overall reduction in speed. All these reasons and more are why I am not going to make a claim regarding the overall reduction in time required for A* to run. I will, though, make the claim that you will see a reduction in overall time.

[^0]: There is a [YouTube video](https://www.youtube.com/watch?v=3fX2kzr_AbQ&t) I made on the topic if you prefer to watch something that covers this topic.
[^1]: I have a post called [A* for Recformer](./astar-for-recformer/) that is an example of using an inadmissable heuristic to find a solution faster.
[^2]: The [code for this project is available on GitHub](https://github.com/bi3mer/simplifying_graphs_for_pathfinding). As of writing this, the repo is not well organized, so apologies.
[^3]: The full description of Kruskal's Algorithm is out of scope for this blog post, but the gif at the already linked Wikipedia article I think is all you need to get the general idea.
[^4]: I am not going into full detail on the exact statistics mainly because the results aren't exactly surprising. The contribution is the method for graph simplification (which again may or may not be original, but I am guessing not original).